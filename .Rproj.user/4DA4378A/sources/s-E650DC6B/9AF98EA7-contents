---
title: "Business Data Analytics"
subtitle: "Solutions for Homework 2"
author: "Kamran Mammadzada"
date: "`r Sys.Date()`"
output:
  html_document:
    keep_md: true
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
editor_options: 
  chunk_output_type: console
---

__Solutions for Homework 2__

The notebook contains solutions for Homework 2 in Business Data Analytics (BDA) Course taught by Rajesh Sharma - University of Tartu.
Original Author of the homework questions - Anna Leontjeva.

__Task:__ Using housing data housing_data.csv predict the price of the house based on other features.

***

First let's load all the necessary data and libraries
```{r warning=FALSE, error=FALSE, message=FALSE}
# load libraries
library(tidyverse)
library(data.table) # helps with data import
library(lubridate) # helps with date manipulation
library(gridExtra) # helps visualize many graphs at once
library(modelr) # provides easy pipeline modeling functions
library(broom) # helps to tidy up model outputs
set.seed(456) # seed to ensure consistency

# my working directory
setwd("C:\\Users\\kamra\\Dropbox\\UT\\2nd\\Business Data Analytics\\Homeworks\\HW2")

# load housing data
df <- read.csv("housing_data.csv")
```

Let's investigate and get to know our dataset
```{r}
str(df)
```

```{r}
summary(df)
```

There are 21 features (variables) in the dataset. It will be difficult to try to predict the price of a house using all features, even though its technically possible. Our goal is to come up with a set of features that can best explain the variation in price thus be able to predict the price with a high accuracy.

# Data Preparation & Visualization

Before we visualize our dataset we will do some data clean up, data type conversion and feature engineering.

```{r}
# convert date and visualize correlation with prices
df$date <- as_date(substr(df$date, start = 0, stop = 8))

```

Let's combine bedrooms and bathrooms into rooms
```{r}
df$rooms <- df$bedrooms + df$bathrooms # generate rooms variable
df$bedrooms <- NULL # remove bedrooms
df$bathrooms <- NULL # remove bathrooms
df$id <- NULL # no need for IDs
```

We will remove lat and long since as an indicator of location we will only use zipcode
```{r}
df$lat <- NULL
df$long <- NULL
```


To better understand our data we will visualize some of the relationship between price and features we think can be predictive.


```{r}
# visualize date and price relation
ggplot(df, aes(x=date, y=price)) +
  geom_point() # date & price
```

The scatter plot shows that there may not be as much of a correlation betwwen date and prices.It will help better understand the relationship and if there's a positive or negative correlation or it is more likely to predict the price. In case of categorical variables such as floors, the variation of prices is a good indicator of predictability.

```{r warning=FALSE}
ggplot(df, aes(x=factor(floors))) +
  geom_histogram(stat = "count") # number of rooms by floors
```

```{r}
ggplot(df, aes(x=factor(floors), y=price)) +
  geom_boxplot() +
  theme_minimal() # floors & price
```

The plot shows that houses with 2.5 rooms have bigger interquartile range (higher variation) but despite that the majority of the houses either have 1 or 2 floors. Based on the boxplot we can also observe that the houses with one or two floors have quite a few outliers, meaning there are a lot of houses with prices far away from mean or even the majority of houses within in the interquartile range.

*** 

Understanding house prices based on geospatial data might also be an important aspect. We will try to visualize the location of houses based on their zipcode.

```{r}
# zipcode library & ggmap will help us do that
library(zipcode)
data(zipcode)
head(zipcode)

zipcode.df <- zipcode %>%
  filter(zip %in% df$zipcode)

zipcode.df$zip <- as.integer(zipcode.df$zip) # convert to integer

# merge zipcode data with df in a new dataset for visualization purposes
df.map <- merge(df, zipcode.df, by.x = 'zipcode', by.y = 'zip') 
```

```{r warning=FALSE}
# library for mapping
library(ggmap)

# getting the map
house_map <- get_map(location = c(lon = mean(df.map$longitude), 
                                  lat = mean(df.map$latitude)), zoom = 10,
                      maptype = "roadmap", scale = 2)

# visualze the house location
ggmap(house_map) +
  geom_point(data=df.map, aes(x=longitude, y=latitude, 
                              fill='red', alpha=0.6, size=price),
             shape = 21) +
  guides(fill=FALSE, alpha=FALSE)
```

All of the zipcodes in the dataset are from Seattle. The houses are quite spread out in Seattle, and also the map show the size of the circle by price. There are areas where prices are more or less the same but some areas have more variability.

Let's visualize the relationship between zipcodes and price and better understand the relationship.

```{r}
ggplot(df) +
  geom_point(aes(x=factor(zipcode), y=price))
```

```{r}
ggplot(df) +
  geom_boxplot(aes(x=factor(zipcode), y=price))
```


Scatter plot and the boxplot shows us that there are a group of zipcodes which have a relatively higher price range.

We will use this feature in our modeling. To keep it simple we will only use the zipcode.


***

Various indicators of house area are concluded inside 6 features

* sqft_living
* sqft_lot
* sqft_above
* sqft_basement
* sqft_living15
* sqft_lot15

We will visualize all of them relative to price.
```{r}
p2 <- ggplot(df, aes(x=sqft_living, y=price)) + geom_point() # sqft_living & price
p3 <- ggplot(df, aes(x=sqft_lot, y=price)) + geom_point() # sqft_lot & price

p9 <- ggplot(df, aes(x=sqft_above, y=price)) + geom_point() # sqft_above & price
p10 <- ggplot(df, aes(x=sqft_basement, y=price)) + geom_point() # sqft_basement & price

p14 <- ggplot(df, aes(x=sqft_living15, y=price)) + geom_point() # sqft_living15 vs price
p15 <- ggplot(df, aes(x=sqft_lot15, y=price)) + geom_point() # sqft_lot15 vs price

grid.arrange(p2, p14, p3, p15, p9, p10)
```

From the scatter plots, it seems that all of the above features have a positive correlation with price, with the exception of sqft_lot and sqft_lot15 where, quite a few of records have no sqft_lot or the ones that have are with less price. We will not use sqft_lot and sqft_lot15 for our price prediction.

***

Every house has rooms and that for sure should influence the price to a certain degree. 
Let's see how it does
```{r}
ggplot(df, aes(x=rooms, y=price)) +
  geom_point() + 
  theme_minimal()
```

There is an obvious correlation with more rooms a house has the higher its price.

***

Year built and year renovated would be an interesting and important feature that can help predict the price. Let's plot it

```{r}
ggplot(df, aes(yr_built, price)) +
  geom_point() # year built vs price
```

```{r}
ggplot(df, aes(yr_renovated, price)) +
  geom_point() # year renovated vs price
```

It seems that if the house is not renovated then the value is 0, which kind of throws off the scaling for this feature. We will ignore it for now and won't use it for our prediction. Also grade and condition variables potentially capture the quality and condition of the house already.

We will use yr_built to generate age and see the correlation of the price vs age

```{r}
df$age <- 2018 - df$yr_built

summary(df$age)
```

It seems that the youngest house is 3 years old while the oldest is 118.

Let's see the correlation with price.
```{r}
ggplot(df, aes(age, price)) +
  geom_point()

cor(df$price, df$age)
```

The plot and the correlation coefficient also reinforces the previous plot. There is almost non-existant effect of age or year built on price.

***

Waterfront and View are categorical variables. Waterfront 0 or 1 indicates whether the house has waterfront or not.

View with values from 0 to 4 shows the quality of the view.

```{r warning=FALSE}
ggplot(df, aes(factor(waterfront))) +
  geom_histogram(stat = "count") +
  theme_minimal() # number of houses by waterfront

```

There are definitely more houses with waterfront than not.

```{r warning=FALSE}
ggplot(df, aes(factor(view))) +
  geom_histogram(stat = "count") +
  theme_minimal() # number of houses by view
```

As with waterfront majority of houses don't have a view or have a poor view.

```{r warning=FALSE}
ggplot(df, aes(factor(condition))) +
  geom_histogram(stat = "count") +
  theme_minimal() # number of houses by condition
```

Most of the houses have an average view

Grades allow categorizing houses by their prestige and quality
```{r warning=FALSE}
ggplot(df, aes(factor(grade))) +
  geom_histogram(stat = "count") +
  theme_minimal() # number of houses by grade
```

Houses are normally distributed when it comes to grade and most of them are average grade which is 7.

***

# Price Prediction

To develop a good model which predicts well we will set up 3 different models with various interactiona and test those models for accuracy. Our accuracy measure will be MSE.

## Train & Test Split
```{r}
sample <- sample(c(TRUE, FALSE), nrow(df), 
                 replace = T, prob = c(0.6,0.4))

# train and test sets
df_train <- df[sample,]
df_test <- df[!sample,]
```


## Modeling

We will use 3 different models utilizing linear regression.

To compare the accuracy, we will also use 2 models using random forest regression.

### Model 1

Our first model will be quite simple, since we will only use the following features that we think will better predict the price

* rooms
* floors
* zipcode
* grade
* age

```{r}
model1 <- lm(price ~ rooms + floors + zipcode + grade + age, data = df_train)

summary(model1)
```

The model1 can only explain 54% of variation in price. (R squared)


### Model 2

Our second model will use the following features

* sqft_living
* sqft_lot
* sqft_above
* sqft_basement
* floors
* condition
* zipcode

```{r}
model2 <- lm(price ~ sqft_living + sqft_lot + 
               sqft_above + sqft_basement + zipcode, data = df_train)

summary(model2)
```

The model1 can only explain ~49% of variation in price. (R squared)

### Model 3

Here we will simply add all of the available features and see how the model performs.

```{r}
model3 <- lm(price ~ date + sqft_living + sqft_lot + floors + waterfront +
             view + condition + grade + sqft_above + sqft_basement + yr_built +
               yr_renovated + zipcode + sqft_living15 + sqft_lot15 + rooms +
               age, data = df_train)

summary(model3)
```

The model 3 shows a 64% R squared, but we will compare the MSE to other models.

### Model 4

We will use features we used in our first model but this time use random forest regression to predict price.

* rooms
* floors
* zipcode
* grade
* age

```{r warning=FALSE}
# load Random Forest algorithm
require(randomForest)

model4 <- randomForest(price ~ rooms + floors +
                         zipcode + grade + age, data=df_train)

summary(model4)
```

The Graph of the random forest

```{r}
plot(model4)
```


### Model 5

Let's now use features from our second model with random forest regression and observe the model.

* sqft_living
* sqft_lot
* sqft_above
* sqft_basement
* floors
* condition
* zipcode

```{r}
model5 <- randomForest(price ~ sqft_living + sqft_lot + 
                         sqft_above + sqft_basement + zipcode, data = df_train)

summary(model5)
```

Graph of model5

```{r}
plot(model5)
```


## Evaluation

Let's evaluate the 5 models we have built.

We will use MSE as a measure of effectiveness.


### Comparing predictions

Let's compare the prediction quality of each of the model using MSE

```{r warning=FALSE}
df_test %>%
  gather_predictions(model1, model2, model3, model4, model5) %>%
  group_by(model) %>%
  summarise(MSE = mean((price-pred)^2))
```

Based on the above table, it does seem that Random Forest regression for our model with the following features proved to have a smaller MSE. This doesn't always mean that in production it will be very effective but taking into account the current task, it does prove to be relatively accurate.




